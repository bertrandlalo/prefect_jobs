import datetime
import itertools
import logging

from iguazu import __version__
from iguazu.cache_validators import ParametrizedValidator
from iguazu.core.flows import PreparedFlow
from iguazu.flows.datasets import GenericDatasetFlow
from iguazu.tasks.common import MergeFilesFromGroups, SlackTask
from iguazu.tasks.galvanic import ApplyCVX, CleanSignal, DetectSCRPeaks, Downsample
from iguazu.core.handlers import garbage_collect_handler, logging_handler
from iguazu.tasks.spectral import BandPowers
from iguazu.tasks.summarize import ExtractFeatures, SummarizePopulation
from iguazu.tasks.unity import ExtractSequences

logger = logging.getLogger(__name__)


class GalvanicFeaturesFlow(PreparedFlow):
    """Extract all galvanic features from a file dataset"""

    REGISTRY_NAME = 'features_galvanic'

    def _build(self, *,
               force=False, workspace_name=None, query=None, alt_query=None,
               **kwargs):
        # Propagate workspace name because we captured it on kwargs
        kwargs['workspace_name'] = workspace_name
        # Force required families: Quetzal workspace must have the following
        # families: (nb: None means "latest" version)
        required_families = dict(
            iguazu=None,
            omi=None,
        )
        families = kwargs.get('families', {}) or {}  # Could be None by default args
        for name in required_families:
            families.setdefault(name, required_families[name])
        kwargs['families'] = families
        # This is the main query that defines the dataset for extracting galvanic
        # features. There is a secondary query because some of the tables may not
        # be available on a new workspace.
        default_query = """\
            SELECT
                id,
                filename
            FROM base
            LEFT JOIN iguazu USING (id)
            LEFT JOIN omi using (id)
            WHERE
                base.state = 'READY' AND                 -- no temporary files
                base.filename LIKE '%.hdf5' AND          -- only HDF5 files
                (
                    iguazu.gsr::json->>'status' IS NULL OR  -- files not fully processed by iguazu on this flow
                    COALESCE(iguazu.gsr::json->>'version', '0') < '{version}' -- or files not processed by this version of iguazu
                ) AND
                iguazu.parents is NULL                   -- files not generated by iguazu
            ORDER BY base.id                             -- always in the same order
        """.format(version=__version__)
        # This secondary, alternative query is defined for the case when a new
        # quetzal workspace is created, and the iguazu.gsr metadata does not even
        # exist. We need to to do this because the iguazu.gsr column does not exist
        # and postgres does not permit querying a non-existent column
        default_alt_query = """\
            SELECT
                id,
                filename
            FROM base
            LEFT JOIN iguazu USING (id)
            WHERE
                base.state = 'READY' AND             -- no temporary files
                base.filename LIKE '%.hdf5'          -- only HDF5 files
            ORDER BY base.id                         -- always in the same order
        """
        kwargs['query'] = query or default_query
        kwargs['alt_query'] = alt_query or default_alt_query

        # Manage connections to other flows
        dataset_flow = GenericDatasetFlow(**kwargs)
        self.update(dataset_flow)
        raw_signals = dataset_flow.terminal_tasks().pop()
        events = raw_signals

        # Instantiate tasks
        clean = CleanSignal(
            # Iguazu task constructor arguments
            signal_column='F',
            warmup_duration=30,
            quality_kwargs=dict(
                sampling_rate=512,
                oa_range=(1e-6, 2000),
                glitch_range=(0.0, 180),
                rejection_window=2,
            ),
            interpolation_kwargs=dict(
                method='pchip',
            ),
            filter_kwargs=dict(
                order=10,
                frequencies=30,
                filter_type='lowpass',
            ),
            scaling_kwargs=dict(
                method='standard',
            ),
            corrupted_maxratio=0.3,
            sampling_rate=512,
            force=force,
            # Prefect task arguments
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        downsample = Downsample(
            # Iguazu task constructor arguments
            sampling_rate=256,
            force=force,
            # Prefect task arguments
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        apply_cvx = ApplyCVX(
            # Iguazu task constructor arguments
            signal_column='F_filtered_clean_inversed_zscored',
            warmup_duration=15,
            threshold_scr=4,
            epoch_size=300,
            epoch_overlap=60,
            force=force,
            # Prefect task arguments
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        detect_scr_peaks = DetectSCRPeaks(
            # Iguazu task constructor arguments
            signal_column='gsr_SCR',
            warmup_duration=15,
            peaks_kwargs=dict(
                width=0.5,
                prominence=.1,
                prominence_window=15,
                rel_height=.5,
            ),
            max_increase_duration=7,  # seconds
            force=force,
            # Prefect task arguments
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        report_sequences = ExtractSequences(
            # Iguazu task constructor arguments
            sequences=None,
            force=force,
            # Prefect task arguments
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        bands = dict(
            # Classic HRV bands      # Number of bins for 60 seconds @ 512 Hz
            # ULF=(0.0001, 0.0033),    # 0 bins (empty!!!)
            VLF=(0.003, 0.04),  # 2 bins
            LF=(0.04, 0.15),  # 6 bins
            HF=(0.15, 0.40),  # 15 bins
            VHF=(0.4, 0.5),  # 6 bins
            # 5Hz wide division of the rest of the signal
            VHF1=(1.0, 5.0),  # 240 bins
            VHF2=(5.0, 10.0),  # 300 bins
            VHF3=(10.0, 15.0),  # 300 bins
            VHF4=(15.0, 20.0),  # 300 bins
            VHF5=(20.0, 25.0),  # 300 bins
            VHF6=(25.0, 30.0),  # 300 bins
        )
        relative_band_powers = BandPowers(
            # Iguazu task constructor arguments
            signal_group='/gsr/timeseries/preprocessed',
            signal_column='F_filtered_clean_inversed',
            epoch_size=60,  # 60 second epochs
            epoch_overlap=59,  # one epoch every second
            bands=bands,
            relative=True,
            # Prefect task arguments
            name='BandPowers_relative',
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        absolute_band_powers = BandPowers(
            # Iguazu task constructor arguments
            signal_group='/gsr/timeseries/preprocessed',
            signal_column='F_filtered_clean_inversed',
            epoch_size=60,  # 60 second epochs
            epoch_overlap=59,  # one epoch every second
            bands=bands,
            relative=False,
            # Prefect task arguments
            name='BandPowers_absolute',
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        extract_features_rbp = ExtractFeatures(
            # Iguazu task constructor arguments
            signals_group="/gsr/timeseries/bandpowers",
            report_group="/unity/sequences_report",
            output_group="/gsr/features/spectral",
            feature_definitions=dict(
                median={
                    "class": "numpy.nanmedian",  # TODO: a geometric mean may be better
                    "columns": ['_'.join(tup) + '_rel' for tup in
                                itertools.product(['F_filtered_clean_inversed'], bands.keys())],
                    # TODO: some way to say all columns
                    "divide_by_duration": False,
                    "empty_policy": 'bad',
                    "drop_bad_samples": True,
                },
            ),
            force=force,
            # Prefect task arguments
            name='ExtractFeatures_rbp',
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        extract_features_abp = ExtractFeatures(
            # Iguazu task constructor arguments
            signals_group="/gsr/timeseries/bandpowers",
            report_group="/unity/sequences_report",
            output_group="/gsr/features/spectral",
            feature_definitions=dict(
                median={
                    "class": "numpy.nanmedian",  # TODO: a geometric mean may be better
                    "columns": ['_'.join(tup) + '_abs' for tup in
                                itertools.product(['F_filtered_clean_inversed'], bands.keys())],
                    # TODO: some way to say all columns
                    "divide_by_duration": False,
                    "empty_policy": 'bad',
                    "drop_bad_samples": True,
                },
            ),
            force=force,
            # Prefect task arguments
            name='ExtractFeatures_abp',
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        extract_features_scr = ExtractFeatures(
            # Iguazu task constructor arguments
            signals_group="/gsr/timeseries/scrpeaks",
            report_group="/unity/sequences_report",
            output_group="/gsr/features/scr",
            feature_definitions=dict(
                rate={
                    "class": "numpy.sum",
                    "columns": ["gsr_SCR_peaks_detected"],
                    "divide_by_duration": True,
                    "empty_policy": 0.0,
                    "drop_bad_samples": True,
                },
                median={
                    "class": "numpy.nanmedian",
                    "columns": ['gsr_SCR_peaks_increase-duration', 'gsr_SCR_peaks_increase-amplitude'],
                    "divide_by_duration": False,
                    "empty_policy": 0.0,
                    "drop_bad_samples": True,
                }
            ),
            force=force,
            # Prefect task arguments
            name='ExtractFeatures_scr',
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        scl_columns_definitions_kwargs = dict(
            columns=['gsr_SCL'],
            divide_by_duration=False,
            empty_policy='bad',
            drop_bad_sample=True,
        )
        extract_features_scl = ExtractFeatures(
            # Iguazu task constructor arguments
            signals_group="/gsr/timeseries/deconvoluted",
            report_group="/unity/sequences_report",
            output_group="/gsr/features/scl",
            feature_definitions={
                "median": {
                    "class": "numpy.nanmedian",
                    **scl_columns_definitions_kwargs,
                },
                "std": {
                    "class": "numpy.nanstd",
                    **scl_columns_definitions_kwargs,
                },
                "ptp": {
                    "class": "numpy.ptp",
                    **scl_columns_definitions_kwargs,
                },
                "linregress": {
                    "custom": "linregress",  # TODO: why is this custom and the other ones are class?
                    **scl_columns_definitions_kwargs,
                },
                "auc": {
                    "custom": "auc",
                    **scl_columns_definitions_kwargs,
                },
            },
            force=force,
            # Prefect task arguments
            name='ExtractFeatures_scl',
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        merge_subject = MergeFilesFromGroups(
            # Iguazu task constructor arguments
            suffix="_gsr",
            status_metadata_key='gsr',
            # Prefect task arguments
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(force=force),
        )
        notify = SlackTask(message='Galvanic feature extraction finished!')

        # Define flow and its task connections
        with self:
            # Galvanic features flow:
            # Signal pre-processing branch: Clean -> CVX -> SCR
            clean_signals = clean.map(signal=raw_signals, events=events)
            clean_signals_256 = downsample.map(signal=clean_signals)
            cvx = apply_cvx.map(clean_signals_256)
            scr = detect_scr_peaks.map(cvx)

            # band power branch
            rel_powers = relative_band_powers.map(signal=clean_signals)
            abs_powers = absolute_band_powers.map(signal=clean_signals)

            # Event handling branch
            sequences_reports = report_sequences.map(events=events)

            # Feature extraction (merge of signal pre-processing and event handling)
            scr_features = extract_features_scr.map(signals=scr, report=sequences_reports)
            scl_features = extract_features_scl.map(signals=cvx, report=sequences_reports)
            rbp_features = extract_features_rbp.map(signals=rel_powers, report=sequences_reports)
            abp_features = extract_features_abp.map(signals=abs_powers, report=sequences_reports)

            # Subject summary
            merged = merge_subject.map(parent=raw_signals,
                                       gsr_timeseries_deconvoluted=cvx,
                                       gsr_features_scr=scr_features,
                                       gsr_features_scl=scl_features,
                                       bp_relative_features=rbp_features,
                                       bp_absolute_features=abp_features,
                                       unity_sequences=sequences_reports)

            # Send slack notification
            notify(upstream_tasks=[merged])

            # TODO: what's the reference task of this flow?

    @staticmethod
    def click_options():
        return GenericDatasetFlow.click_options()


class GalvanicSummaryFlow(PreparedFlow):
    """Collect all galvanic features in a single CSV file"""

    REGISTRY_NAME = 'summarize_galvanic'

    def _build(self, *,
               workspace_name=None, query=None, alt_query=None,
               **kwargs):

        # Manage parameters
        kwargs = kwargs.copy()
        # Propagate workspace name because we captured it on kwargs
        kwargs['workspace_name'] = workspace_name
        # Force required families: Quetzal workspace must have the following
        # families: (nb: None means "latest" version)
        required_families = dict(
            iguazu=None,
            omi=None,
        )
        families = kwargs.get('families', {}) or {}  # Could be None by default args
        for name in required_families:
            families.setdefault(name, required_families[name])
        kwargs['families'] = families
        # This is the main query that defines the dataset for merging the galvanic
        # features. There is a secondary query because some of the tables may not
        # be available on a new workspace.
        default_query = """\
            SELECT
                id,
                filename
            FROM base
            LEFT JOIN iguazu USING (id)
            LEFT JOIN omi using (id)
            WHERE
                base.state = 'READY' AND                    -- no temporary files
                base.filename LIKE '%_gsr.hdf5' AND         -- only HDF5 files
                base.filename NOT LIKE '%_gsr_gsr.hdf5' AND -- remove incorrect cases where we processed twice
                COALESCE(iguazu."MergeFilesFromGroups", '{{}}')::json->>'state' = 'SUCCESS' AND -- Only files whose mergefilefromgroups was successful
                COALESCE(iguazu."MergeFilesFromGroups", '{{}}')::json->>'version' = '{version}'     -- On this particular iguazu version
            ORDER BY base.id                                -- always in the same order
        """.format(version=__version__)  # Note the {{}} to avoid formatting the coalesce terms
        # There is no secondary query because this flow only makes sense *after*
        # the galvanic extract features flow has run
        default_alt_query = None
        kwargs['query'] = query or default_query
        kwargs['alt_query'] = alt_query or default_alt_query

        # Manage connections to other flows
        dataset_flow = GenericDatasetFlow(**kwargs)
        self.update(dataset_flow)
        features_files = dataset_flow.terminal_tasks().pop()

        # instantiate tasks
        merge_population = SummarizePopulation(
            # Iguazu task constructor arguments
            groups={'gsr_features_scr': None,  # TODO: I dont understand why use _ instead of / since we are talking about groups
                    'gsr_features_scl': None,
                    'bp_relative_features': None,
                    'bp_absolute_features': None,},
            filename='galvanic_summary',
            # Prefect task arguments
            state_handlers=[garbage_collect_handler, logging_handler],
            cache_for=datetime.timedelta(days=7),
            cache_validator=ParametrizedValidator(),
        )
        notify = SlackTask(message='Galvanic feature summarization finished!')

        with self:
            population_summary = merge_population(features_files)

            # Send slack notification
            notify(upstream_tasks=[population_summary])

            # TODO: what's the reference task of this flow?

    @staticmethod
    def click_options():
        return GenericDatasetFlow.click_options()
